{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Digit Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential # type: ignore\n",
    "from tensorflow.keras.layers import Dense  # type: ignore\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid  # type: ignore\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.random.set_seed(1234)\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Problem Statement\n",
    "\n",
    "In this project, we will build a neural network to recognize ten handwritten digits, `0-9`. This is a multiclass classification task where one of n choices is selected. Automated handwritten digit recognition is widely used today, from recognizing zip codes (postal codes) on mail envelopes to identifying amounts written on bank checks.\n",
    "\n",
    "## 2. Dataset\n",
    "\n",
    "We will start by loading the dataset for this task.\n",
    "\n",
    "- The first part of the training set is a 1000 x 400 dimensional vector `X` that contains the input features for the training set:\n",
    "    - Each training example is a 20x20 pixels grayscale image of the digit.\n",
    "    - Each pixel is represented by a `1` or a `0` number indicating the intensity at that location.\n",
    "    - The 20 by 20 grid of pixels is unrolled into a 400-dimensional vector.\n",
    "\n",
    "- The second part of the training set is a 1000 x 1 dimensional vector `y` that contains labels for the training set:\n",
    "    - `y = 0` if the image is of the digit `0`, `y = 4` if the image is of the digit `4` and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "X, y = load_data() # _ * 400, _ * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(type(X))\n",
    "print(type(y))\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing the Data\n",
    "\n",
    "Let's begin by visualizing a subset of the training set.\n",
    "\n",
    "In the cell below, the code randomly selects 64 rows from `X`, maps each row back to a 20x20 pixels grayscale image and displays the images together.\n",
    "\n",
    "The label for each image is displayed above the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m, n = X.shape\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(10, 5))\n",
    "fig.tight_layout(pad=1)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    random_index = np.random.randint(m)\n",
    "\n",
    "    X_random_reshaped = X[random_index].reshape((20, 20))\n",
    "    ax.imshow(X_random_reshaped, cmap='gray')\n",
    "\n",
    "    ax.set_title(y[random_index])\n",
    "    ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Representation\n",
    "\n",
    "The neural network we are building is shown in the figure below.\n",
    "\n",
    "This has the following structure:\n",
    "- Two dense layers with ReLU activations\n",
    "- An output layer with a linear activation\n",
    "\n",
    "<center>\n",
    "    <img src=\"./Images/model-representation.png\" alt=\"Model Representation\" height=\"300\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. TensorFlow Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(400,)),  # type: ignore\n",
    "        Dense(units=20, activation=\"relu\", name=\"L1\"),\n",
    "        Dense(units=16, activation=\"relu\", name=\"L2\"),\n",
    "        Dense(units=11, activation=\"linear\", name=\"L3\"),\n",
    "    ],\n",
    "    name=\"ndv_model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "[L1, L2, L3] = model.layers\n",
    "\n",
    "W1, b1 = L1.get_weights()\n",
    "W2, b2 = L2.get_weights()\n",
    "W3, b3 = L3.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "print(f\"W1 shape: {W1.shape}, b1 shape: {b1.shape}\")\n",
    "print(f\"W2 shape: {W2.shape}, b2 shape: {b2.shape}\")\n",
    "print(f\"W3 shape: {W3.shape}, b3 shape: {b3.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 2.3811\n",
      "Epoch 2/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.2869\n",
      "Epoch 3/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.2119 \n",
      "Epoch 4/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.1227\n",
      "Epoch 5/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.0151\n",
      "Epoch 6/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.8912\n",
      "Epoch 7/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.7608 \n",
      "Epoch 8/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.6312 \n",
      "Epoch 9/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.5070 \n",
      "Epoch 10/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.3904\n",
      "Epoch 11/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.2833\n",
      "Epoch 12/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.1859\n",
      "Epoch 13/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.0983 \n",
      "Epoch 14/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.0187\n",
      "Epoch 15/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.9465\n",
      "Epoch 16/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.8805 \n",
      "Epoch 17/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.8189 \n",
      "Epoch 18/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.7614\n",
      "Epoch 19/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.7090\n",
      "Epoch 20/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.6604\n",
      "Epoch 21/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.6157 \n",
      "Epoch 22/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.5744 \n",
      "Epoch 23/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.5358\n",
      "Epoch 24/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.4998 \n",
      "Epoch 25/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4658\n",
      "Epoch 26/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.4347 \n",
      "Epoch 27/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4044\n",
      "Epoch 28/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3763\n",
      "Epoch 29/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3494\n",
      "Epoch 30/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3242  \n",
      "Epoch 31/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3002\n",
      "Epoch 32/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2781 \n",
      "Epoch 33/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2570 \n",
      "Epoch 34/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2372  \n",
      "Epoch 35/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2188 \n",
      "Epoch 36/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2019\n",
      "Epoch 37/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1860 \n",
      "Epoch 38/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1713 \n",
      "Epoch 39/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1578\n",
      "Epoch 40/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1456 \n",
      "Epoch 41/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1343 \n",
      "Epoch 42/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1240\n",
      "Epoch 43/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1149 \n",
      "Epoch 44/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1064 \n",
      "Epoch 45/45\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0989 \n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  # type: ignore\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),  # type: ignore\n",
    ")\n",
    "\n",
    "history = model.fit(X, y, epochs=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL 0.2429 0.0989\n",
    "model.save(\"ndv_model_285t__45ep.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD MODEL\n",
    "model = tf.keras.models.load_model(\"ndv_model_285t.keras\")  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "[L1, L2, L3] = model.layers\n",
    "\n",
    "W1, b1 = L1.get_weights()\n",
    "W2, b2 = L2.get_weights()\n",
    "W3, b3 = L3.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.inf) # type: ignore\n",
    "\n",
    "print(W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "plot_loss_tf(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction \n",
    "To make a prediction, use Keras `predict`. Below, X[1015] contains an image of a two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# image_of_seven = X[0]\n",
    "\n",
    "image_of_two = np.array(\n",
    "    [\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "    ]\n",
    ")\n",
    "\n",
    "display_digit(image_of_two)\n",
    "\n",
    "prediction = model.predict(image_of_two.reshape(1, 400))\n",
    "\n",
    "print(f\"Predicting a 'Two': {prediction}\")\n",
    "print(f\"Largest prediction index: {np.argmax(prediction)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The largest output is prediction[2], indicating the predicted digit is a '2'. If the problem only requires a selection, that is sufficient. Use NumPy [argmax](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) to select it. If the problem requires a probability, a softmax is required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "prediction_prb = tf.nn.softmax(prediction)\n",
    "probabilities = \" \".join([f\"{prob:2.3f}\" for prob in prediction_prb[0]])\n",
    "\n",
    "print(f\"Predicting a 'Two' probability vector: {probabilities}\")\n",
    "print(f\"Total of predictions: {np.sum(prediction_prb):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "yhat = np.argmax(prediction_prb)\n",
    "print(f\"np.argmax(prediction_prb): {yhat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the predictions `vs` the labels for a random sample of 8 digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "m, n = X.shape\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(10, 5))\n",
    "fig.tight_layout(pad=1)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    random_index = np.random.randint(m)\n",
    "\n",
    "    X_random_reshaped = X[random_index].reshape((20, 20))\n",
    "\n",
    "    ax.imshow(X_random_reshaped, cmap='gray')\n",
    "\n",
    "    prediction = model.predict(X[random_index].reshape(1, 400))\n",
    "    prediction_prb = tf.nn.softmax(prediction)\n",
    "    yhat = np.argmax(prediction_prb)\n",
    "\n",
    "    ax.set_title(f\"{y[random_index]}, {yhat} -> {np.max(prediction_prb)*100:2.3f}%\", fontsize=10)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "print(f\"{display_errors(model, X, y, True)} errors out of {len(X)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "## Thanks for making it this far! Not everyone does!"
   ]
  }
 ],
 "metadata": {
  "dl_toc_settings": {
   "rndtag": "89367"
  },
  "kernelspec": {
   "display_name": "TFVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
